# CS 6301.M02
# NLP
# Project1: Building a Corpus
# Kevin Tabesh
# Net-id: KXT170430

# ===========================
import os
import pickle
import string
from urllib import request

import nltk
from bs4 import BeautifulSoup
import re

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from sklearn.feature_extraction.text import CountVectorizer
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer


source_file_list =[]

if not os.path.exists('data'):
    dirpath = 'data'
    os.mkdir(dirpath)
if not os.path.exists('data/source'):
    dirpath2 = 'data/source'
    os.mkdir(dirpath2)
if not os.path.exists('data/output'):
    dirpath3 = 'data/output'
    os.mkdir(dirpath3)

def start():

    url = 'https://en.wikipedia.org/wiki/Interstellar_(film)'
    html = request.urlopen(url).read().decode('utf8')
    soup = BeautifulSoup(html, features="html.parser")
    # kill all script and style elements
    for script in soup(["script", "style"]):
        script.extract()    # rip it out
    # extract text
    text = soup.get_text()

    text_chunks = [chunk for chunk in text.splitlines() if not re.match(r'^\s*$', chunk)]
    # for i, chunk in enumerate(text_chunks):
        # sen = sent_tokenize(chunk)

        # print(i + 1, chunk)



    # new_text = re.sub('\n+',' ',text_chunks)
    # # print(new_text)
    # sentences = sent_tokenize(new_text)
    # print(sentences)
    url_list =[]
    counter = 0
    for link in soup.find_all('a'):
        counter += 1
        if counter > 1000:
            break
        l = str(link.get('href'))
        if counter>5:
            if len(url_list)<10:
                if l.startswith('/wiki'):
                    url_list.append('https://en.wikipedia.org' +l)

            elif len(url_list)<15:
                if l.startswith( 'https://www.hollywood'):
                    url_list.append(l)

    for url_t in url_list:

        splitted_url = url_t.split('/')

        file_name = splitted_url[len(splitted_url)-1]
        extract_url_content(url_t, file_name)


    create_output_files()





def extract_url_content(url, file_name):
    source_file_list.append(file_name)
    html = request.urlopen(url).read().decode('utf8')
    soup = BeautifulSoup(html, features="html.parser")
    for script in soup(["script", "style"]):
        script.extract()

    text = soup.get_text()

    text_chunks = [chunk for chunk in text.splitlines() if not re.match(r'^\s*$', chunk)]
    new_text = ''
    for i, chunk in enumerate(text_chunks):
        new_text += chunk
    # Creating File
    directory = 'data/source'
    filename = file_name+'.txt'
    file_path = os.path.join(directory, filename)
    f = open(file_path, "a")
    f.write(new_text)
    f.close()

def create_output_files():
    for src in source_file_list:
        # print(src)
        with open("data/source/"+src+'.txt') as f:
            contents = f.read()
            contents = contents.replace('\t', ' ')
            contents = re.sub('[![]#]', '', contents)

            pickle.dump(contents, open("data/output/"+src+'.pickle', "wb"))

    extract_imp_terms()

def extract_imp_terms():
    for item in source_file_list:
        text = pickle.load(open("data/output/" + item + '.pickle', "rb"))
        # text = re.sub("[^-9A-Za-z ]", "", text).lower()
        # stop = stopwords.words("english")
        # tokens = [word for word in (token for token in word_tokenize(text)) if word not in stop]
        # lmtzr = nltk.WordNetLemmatizer()
        #
        # preprocessed_text = ' '.join([lmtzr.lemmatize(word) for word in tokens])
        tokenizer = Tokenizer(num_words=100, oov_token = "<OOV>")
        tokenizer.fit_on_texts(text)
        word_index = tokenizer.word_index
        sequences = tokenizer.texts_to_sequences(text)
        # print(sequences)
        # print(preprocessed_text)
        # return preprocessed_text
        # pk = pk.lower()
        # tokens = word_tokenize(pk)
        # tokens_t = [t for t in tokens if t.isalpha() and
        #             t not in stopwords.words('english') and
        #             len(t) > 5]
        # # print(tokens_t)
        #
        # vectorizer = CountVectorizer.fit(tokens_t)
        # print(vectorizer)
            # con = [t for t in contents if t.isalpha() and
            #             t not in stopwords.words('english') ]
            # s = sent_tokenize(con)

if __name__ == '__main__':
    start()

# See PyCharm help at https://www.jetbrains.com/help/pycharm/
